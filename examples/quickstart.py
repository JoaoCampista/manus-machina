"""
Manus Machina v4.0 - Quick Start Example

This example demonstrates the core features of Manus Machina v4.0:
- Session Management
- State Management with scopes
- LiteLLM Integration (100+ LLM providers)
- Artifacts
- Domain Events
"""

import asyncio
import os
from manus_machina.session import InMemorySessionService
from manus_machina.llm.litellm_client import LiteLLMClient, LLMMessage
from manus_machina.artifacts.artifact import Artifact, ArtifactType


async def main():
    print("🚀 Manus Machina v4.0 - Quick Start\n")
    
    # 1. Create Session Service
    print("1️⃣ Creating session service...")
    session_service = InMemorySessionService()
    
    # 2. Create a new session
    print("2️⃣ Creating new session...")
    session = await session_service.create_session(
        user_id="demo_user",
        app_name="quickstart_demo",
        tags=["demo", "tutorial"]
    )
    print(f"   ✅ Session created: {session.id}\n")
    
    # 3. Set state (different scopes)
    print("3️⃣ Setting state with different scopes...")
    
    # Session state (specific to this conversation)
    session.set_state("topic", "quantum computing")
    session.set_state("difficulty", "beginner")
    print("   ✅ Session state: topic='quantum computing', difficulty='beginner'")
    
    # User state (persists across sessions)
    session.set_user_state("preferred_language", "en")
    session.set_user_state("expertise_level", "intermediate")
    print("   ✅ User state: language='en', expertise='intermediate'")
    
    # App state (global configuration)
    session.set_app_state("max_response_length", 500)
    print("   ✅ App state: max_response_length=500\n")
    
    # 4. Create LLM Client (supports 100+ providers)
    print("4️⃣ Creating LLM client...")
    
    # Check if API key is available
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("   ⚠️  GOOGLE_API_KEY not found. Using mock response.")
        mock_response = True
    else:
        mock_response = False
        llm = LiteLLMClient(
            model="gemini/gemini-2.0-flash-exp",
            temperature=0.7,
            api_key=api_key
        )
        print(f"   ✅ LLM client created: gemini/gemini-2.0-flash-exp\n")
    
    # 5. Generate response using state
    print("5️⃣ Generating response...")
    
    topic = session.get_state("topic")
    difficulty = session.get_state("difficulty")
    
    if not mock_response:
        messages = [
            LLMMessage(
                role="system",
                content="You are a helpful AI assistant."
            ),
            LLMMessage(
                role="user",
                content=f"Explain {topic} for a {difficulty} level audience in 3 paragraphs."
            )
        ]
        
        response = await llm.generate(messages)
        content = response.content
        tokens = response.total_tokens
        
        print(f"   ✅ Response generated ({tokens} tokens)\n")
        print(f"   📝 Response:\n{content}\n")
    else:
        content = f"This is a mock explanation of {topic} for {difficulty} level.\n\n" \
                  "In a real scenario, this would be generated by an LLM like GPT-4, " \
                  "Claude, or Gemini.\n\nSet GOOGLE_API_KEY environment variable to " \
                  "see real LLM responses."
        print(f"   ✅ Mock response generated\n")
        print(f"   📝 Response:\n{content}\n")
    
    # 6. Create an artifact
    print("6️⃣ Creating artifact...")
    artifact = Artifact(
        type=ArtifactType.DOCUMENT_MARKDOWN,
        title=f"{topic.title()} Explanation",
        description=f"A {difficulty}-level explanation of {topic}",
        content=content,
        session_id=session.id,
        agent_name="ExplainerAgent",
        tags=["explanation", topic.replace(" ", "-")]
    )
    
    session.add_artifact(artifact)
    print(f"   ✅ Artifact created: {artifact.title}")
    print(f"   📄 Suggested filename: {artifact.get_suggested_filename()}\n")
    
    # 7. Update session
    print("7️⃣ Updating session...")
    await session_service.update_session(session)
    print("   ✅ Session updated\n")
    
    # 8. View session summary
    print("8️⃣ Session Summary:")
    print(f"   📊 Session ID: {session.id}")
    print(f"   👤 User ID: {session.metadata.user_id}")
    print(f"   📱 App Name: {session.metadata.app_name}")
    print(f"   🏷️  Tags: {', '.join(session.metadata.tags)}")
    print(f"   📝 Events: {len(session.events)}")
    print(f"   🎨 Artifacts: {len(session.artifacts)}")
    print(f"   🔑 State keys: {len(session.state)}")
    print(f"   ⏰ Created: {session.created_at.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   ⏰ Updated: {session.updated_at.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 9. Demonstrate state scopes
    print("9️⃣ State Scopes:")
    print(f"   Session state: {list(session.state.get_session_state().keys())}")
    print(f"   User state: {list(session.state.get_user_state().keys())}")
    print(f"   App state: {list(session.state.get_app_state().keys())}\n")
    
    print("✅ Quick start completed successfully!")
    print("\n📚 Next steps:")
    print("   - Explore more examples in the examples/ directory")
    print("   - Read the documentation: https://joaocampista.github.io/manus-machina/")
    print("   - Try different LLM providers (OpenAI, Anthropic, Cohere, etc.)")
    print("   - Implement your own agents and workflows")


if __name__ == "__main__":
    asyncio.run(main())
